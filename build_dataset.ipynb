{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import find_ped_img_idx, read_gt, HOG_descriptor\n",
    "from utils import one_hot_encoder, built_codebook\n",
    "from skimage.transform import resize\n",
    "from skimage.color import rgb2luv, rgb2gray\n",
    "from skimage.feature import local_binary_pattern\n",
    "\n",
    "import cv2 as cv\n",
    "import numpy as np\n",
    "import tqdm\n",
    "import time\n",
    "import pickle\n",
    "import os\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Functions to be used\n",
    "\n",
    "def bag_of_features(img_kmeans, bb_list, mode='normal'):\n",
    "    \n",
    "    if not isinstance(bb_list[0], list) and not isinstance(bb_list[0], tuple):\n",
    "        bb_list = [bb_list]\n",
    "    \n",
    "    features = []\n",
    "    \n",
    "    for bb in bb_list:\n",
    "        x, y, dx, dy = bb[2:]\n",
    "        \n",
    "        if mode=='normal':\n",
    "            pos = img_kmeans[y:y+dy, x:x+dx, :]\n",
    "            pos = np.reshape(pos, (pos.shape[0]*pos.shape[1], -1))\n",
    "        elif mode=='hog':\n",
    "            y1 = math.floor((y-8)/8)\n",
    "            if y1<0:\n",
    "                y1 = 0\n",
    "            y2 = math.ceil((y+dy-8)/8)\n",
    "            x1 = math.floor((x-8)/8)\n",
    "            if x1<0:\n",
    "                x1 = 0\n",
    "            x2 = math.ceil((x+dx-8)/8)\n",
    "\n",
    "            if y1==y2:\n",
    "                y2 += 1\n",
    "            if x1==x2:\n",
    "                x2 += 1\n",
    "\n",
    "            pos = img_kmeans[y1:y2, x1:x2, :]\n",
    "            pos = np.reshape(pos, (pos.shape[0]*pos.shape[1], -1))\n",
    "        \n",
    "        feature = np.sum(pos, axis=0, keepdims=True)\n",
    "        feature = feature/np.sum(feature)\n",
    "        features += [feature]\n",
    "    \n",
    "    return np.concatenate(features, axis=0)\n",
    "\n",
    "def propose_negative(bb_list, ratio=1):\n",
    "    \n",
    "    bb_propose = []\n",
    "    coord = [bb[2:] for bb in bb_list]\n",
    "    \n",
    "    for bb in bb_list:\n",
    "        x, y, dx, dy = bb[2:]\n",
    "        for it in range(ratio):\n",
    "            #pdb.set_trace()\n",
    "            break_flag = False\n",
    "            while True:\n",
    "                x1 = np.random.randint(640)\n",
    "                y1 = np.random.randint(430)\n",
    "                dx1 = np.random.randint(300)+1\n",
    "                dy1 = np.random.randint(150)+1\n",
    "                \n",
    "                if x1+dx1>=640-8 or y1+dy1>=430-8:\n",
    "                    continue\n",
    "                if not is_overlap(coord, (x1, y1, dx1, dy1)):\n",
    "                    break\n",
    "            '''\n",
    "            if np.random.randint(2):\n",
    "                while True:\n",
    "                    x1 = x+np.random.randint(dx)-math.floor(dx/2)\n",
    "                    y1 = y+np.random.randint(dy)-math.floor(dy/2)\n",
    "                    dx1 = np.random.randint(dx*2)\n",
    "                    dy1 = np.random.randint(dy*2)\n",
    "                    if x1+dx1>=430 or y1+dy1>=640:\n",
    "                        continue\n",
    "                    if not is_overlap(coord, (x1, y1, dx1, dy1)):\n",
    "                        break\n",
    "            else:\n",
    "                while True:\n",
    "                    x1 = np.random.randint(430)\n",
    "                    y1 = y+np.random.randint(640)\n",
    "                    dx1 = np.random.randint(150)\n",
    "                    dy1 = np.random.randint(300)\n",
    "                    if x1+dx1>=430 or y1+dy1>=640:\n",
    "                        continue\n",
    "                    if not is_overlap(coord, (x1, y1, dx1, dy1)):\n",
    "                        break\n",
    "            '''\n",
    "            bb_propose += [[bb[0], bb[1], x1, y1, dx1, dy1]]\n",
    "    return bb_propose\n",
    "\n",
    "def compute_overlap(coord1, coord2, h=430, w=640):\n",
    "    x1, y1, dx1, dy1 = coord1\n",
    "    x2, y2, dx2, dy2 = coord2\n",
    "    img1 = np.zeros((h, w))\n",
    "    img1[y1:y1+dy1, x1:x1+dx1] = 1\n",
    "    img2 = np.zeros((h, w))\n",
    "    img2[y2:y2+dy2, x2:x2+dx2] = 1\n",
    "\n",
    "    intersection = img1 * img2\n",
    "    return intersection.sum()/img1.sum()\n",
    "\n",
    "def is_overlap(coord1s, coord2, h=430, w=640):\n",
    "    is_overlap = False\n",
    "    for coord1 in coord1s:\n",
    "        if compute_overlap(coord1, coord2)>=0.5:\n",
    "            is_overlap = True\n",
    "            break\n",
    "    return is_overlap\n",
    "\n",
    "def unpackbits(x, num_bits):\n",
    "    xshape = list(x.shape)\n",
    "    x = x.reshape([-1,1])\n",
    "    to_and = 2**np.arange(num_bits).reshape([1,num_bits])\n",
    "    return (x & to_and).astype(bool).astype(int).reshape(xshape + [num_bits])\n",
    "\n",
    "def kmeans_for_img(kmeans, img):\n",
    "    h, w, ch = img.shape\n",
    "    img = np.reshape(img, (h*w, ch))\n",
    "    img = kmeans.predict(img)\n",
    "    img = one_hot_encoder(img, 64)\n",
    "    img = np.reshape(img, (h, w, 64))\n",
    "    return img"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training K-means HoG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 402/402 [05:06<00:00,  1.17it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time:  2.7138993740081787\n",
      "score:  168497.10207500207\n"
     ]
    }
   ],
   "source": [
    "img_ped_idx = find_ped_img_idx()\n",
    "hog_features = []\n",
    "for idx in tqdm.tqdm(img_ped_idx):\n",
    "    img_name = str(idx)\n",
    "    if len(img_name)==1:\n",
    "        img_name = '00'+img_name+'.jpg'\n",
    "    elif len(img_name)==2:\n",
    "        img_name = '0'+img_name+'.jpg'\n",
    "    elif len(img_name)==3:\n",
    "        img_name = img_name+'.jpg'\n",
    "    img_path = './img1/' + img_name\n",
    "    \n",
    "    img = cv.imread(img_path)\n",
    "    fd, hog_img = HOG_descriptor(img)\n",
    "    h, w, ch = fd.shape\n",
    "    fd = np.reshape(fd, (h*w, ch))\n",
    "    hog_features += [fd]\n",
    "hog_features = np.concatenate(hog_features, axis=0)\n",
    "\n",
    "start = time.time()\n",
    "kmeans_hog = built_codebook(hog_features)\n",
    "end = time.time()\n",
    "print('Time: ', end-start)\n",
    "print('score: ', -kmeans_hog.score(hog_features))\n",
    "\n",
    "np.save('./Models/hog_features.npy', hog_features)\n",
    "np.save('./Models/hog_codebook.npy', kmeans_hog.cluster_centers_)\n",
    "pickle.dump(kmeans_hog, open('./Models/kmeans_hog.sav', 'wb'))\n",
    "\n",
    "# Load\n",
    "# loaded_model = pickle.load(open(filename, 'rb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training K-means LBP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 48%|████▊     | 191/402 [00:52<00:53,  3.92it/s]"
     ]
    }
   ],
   "source": [
    "img_ped_idx = find_ped_img_idx()\n",
    "lbp_features = []\n",
    "for idx in tqdm.tqdm(img_ped_idx):\n",
    "    img_name = str(idx)\n",
    "    if len(img_name)==1:\n",
    "        img_name = '00'+img_name+'.jpg'\n",
    "    elif len(img_name)==2:\n",
    "        img_name = '0'+img_name+'.jpg'\n",
    "    elif len(img_name)==3:\n",
    "\n",
    "        img_name = img_name+'.jpg'\n",
    "    img_path = './img1/' + img_name\n",
    "    \n",
    "    img = cv.imread(img_path)\n",
    "    img = resize(img, output_shape=(430, 640), \n",
    "                 anti_aliasing=True, mode='reflect')\n",
    "    img = rgb2gray(img)\n",
    "    img_lbp = np.ndarray.astype(local_binary_pattern(img, P=8*3, R=3), np.uint32)\n",
    "    img_lbp = img_lbp[::2, ::2]\n",
    "    lbp_feature = unpackbits(img_lbp, num_bits=24)\n",
    "    h, w, ch = lbp_feature.shape\n",
    "    lbp_feature = np.reshape(lbp_feature, (h*w, ch))\n",
    "    lbp_features += [lbp_feature]\n",
    "        \n",
    "lbp_features = np.concatenate(lbp_features, axis=0)\n",
    "\n",
    "start = time.time()\n",
    "kmeans_lbp = built_codebook(lbp_features)\n",
    "end = time.time()\n",
    "print('Time: ', end-start)\n",
    "print('score: ', -kmeans_lbp.score(lbp_features))\n",
    "\n",
    "np.save('./Models/lbp_features.npy', lbp_features)\n",
    "np.save('./Models/lbp_codebook.npy', kmeans_lbp.cluster_centers_)\n",
    "pickle.dump(kmeans_lbp, open('./Models/kmeans_lbp.sav', 'wb'))\n",
    "\n",
    "# Load\n",
    "# loaded_model = pickle.load(open(filename, 'rb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training K-means LUV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_ped_idx = find_ped_img_idx()\n",
    "luv_features = []\n",
    "for idx in tqdm.tqdm(img_ped_idx):\n",
    "    img_name = str(idx)\n",
    "    if len(img_name)==1:\n",
    "        img_name = '00'+img_name+'.jpg'\n",
    "    elif len(img_name)==2:\n",
    "        img_name = '0'+img_name+'.jpg'\n",
    "    elif len(img_name)==3:\n",
    "        img_name = img_name+'.jpg'\n",
    "    img_path = './img1/' + img_name\n",
    "    \n",
    "    img = cv.imread(img_path)\n",
    "    img = resize(img, output_shape=(430, 640), \n",
    "                 anti_aliasing=True, mode='reflect')\n",
    "    img_luv = rgb2luv(img)\n",
    "    img_luv = img_luv[::2, ::2]\n",
    "    \n",
    "    h, w, ch = img_luv.shape\n",
    "    img_luv = np.reshape(img_luv, (h*w, ch))\n",
    "    luv_features += [img_luv]\n",
    "        \n",
    "luv_features = np.concatenate(luv_features, axis=0)\n",
    "\n",
    "start = time.time()\n",
    "kmeans_luv = built_codebook(luv_features)\n",
    "end = time.time()\n",
    "print('Time: ', end-start)\n",
    "print('score: ', -kmeans_luv.score(luv_features))\n",
    "\n",
    "np.save('./Models/luv_features.npy', luv_features)\n",
    "np.save('./Models/luv_codebook.npy', kmeans_luv.cluster_centers_)\n",
    "pickle.dump(kmeans_luv, open('./Models/kmeans_luv.sav', 'wb'))\n",
    "\n",
    "# Load\n",
    "# loaded_model = pickle.load(open(filename, 'rb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kmeans_hog = pickle.load(open('./Models/kmeans_hog.sav', 'rb'))\n",
    "kmeans_lbp = pickle.load(open('./Models/kmeans_lbp.sav', 'rb'))\n",
    "kmeans_luv = pickle.load(open('./Models/kmeans_luv.sav', 'rb'))\n",
    "gt = read_gt('./gt/gt.txt')\n",
    "\n",
    "for bb in gt:\n",
    "    bb[2] = math.floor(bb[2]/2)\n",
    "    bb[3] = math.floor(bb[3]/2)\n",
    "    bb[4] = math.ceil(bb[4]/2)\n",
    "    bb[5] = math.ceil(bb[5]/2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "neg_pos_ratio = 4\n",
    "\n",
    "img_ped_idx = find_ped_img_idx(skip=1)\n",
    "features_pd = []\n",
    "features_bg = []\n",
    "for idx in tqdm.tqdm(img_ped_idx):\n",
    "    \n",
    "    img_name = str(idx)\n",
    "    if len(img_name)==1:\n",
    "        img_name = '00'+img_name+'.jpg'\n",
    "    elif len(img_name)==2:\n",
    "        img_name = '0'+img_name+'.jpg'\n",
    "    elif len(img_name)==3:\n",
    "        img_name = img_name+'.jpg'\n",
    "    img_path = './img1/' + img_name\n",
    "    \n",
    "    img = cv.imread(img_path)\n",
    "    img = resize(img, output_shape=(430, 640), \n",
    "                 anti_aliasing=True, mode='reflect')\n",
    "    \n",
    "    bb_pos = [tmp for tmp in gt if tmp[0]==idx]\n",
    "    bb_neg = propose_negative(bb_pos, ratio=neg_pos_ratio)\n",
    "    \n",
    "    feature_pd = []\n",
    "    feature_bg = []\n",
    "    \n",
    "    # HoG features\n",
    "    fd, _ = HOG_descriptor(img)\n",
    "    fd = kmeans_for_img(kmeans_hog, fd)\n",
    "    \n",
    "    feature = bag_of_features(fd, bb_pos, mode='hog')\n",
    "    feature_pd += [feature]\n",
    "    feature = bag_of_features(fd, bb_neg, mode='hog')\n",
    "    feature_bg += [feature]\n",
    "    \n",
    "    # LBP features\n",
    "    img_gray = rgb2gray(img)\n",
    "    img_lbp = np.ndarray.astype(local_binary_pattern(img_gray, P=8*3, R=3), np.uint32)\n",
    "    lbp_feature = unpackbits(img_lbp, num_bits=24)\n",
    "    lbp_feature = kmeans_for_img(kmeans_lbp, lbp_feature)\n",
    "    \n",
    "    feature = bag_of_features(lbp_feature, bb_pos)\n",
    "    feature_pd += [feature]\n",
    "    feature = bag_of_features(lbp_feature, bb_neg)\n",
    "    feature_bg += [feature]\n",
    "    \n",
    "    # LUV features\n",
    "    img_luv = rgb2luv(img)\n",
    "    img_luv = kmeans_for_img(kmeans_luv, img_luv)\n",
    "    \n",
    "    feature = bag_of_features(img_luv, bb_pos)\n",
    "    feature_pd += [feature]\n",
    "    feature = bag_of_features(img_luv, bb_neg)\n",
    "    feature_bg += [feature]\n",
    "    \n",
    "    # Concatenate the features\n",
    "    feature_pd = np.concatenate(feature_pd, axis=1)\n",
    "    feature_bg = np.concatenate(feature_bg, axis=1)\n",
    "    \n",
    "    features_pd += [feature_pd]\n",
    "    features_bg += [feature_bg]\n",
    "    \n",
    "features_pd = np.concatenate(features_pd, axis=0)\n",
    "features_pd = np.repeat(features_pd, neg_pos_ratio, axis=0)\n",
    "features_bg = np.concatenate(features_bg, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = np.concatenate([features_pd, features_bg], axis=0)\n",
    "t_train = np.zeros((features_pd.shape[0]+features_bg.shape[0], ))\n",
    "t_train[0:features_pd.shape[0], ] = 1\n",
    "\n",
    "np.save('./Dataset/x_train.npy', x_train)\n",
    "np.save('./Dataset/t_train.npy', t_train)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
